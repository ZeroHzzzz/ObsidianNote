这两个概念和[[长尾学习和logit调整#Logit调整的统计视角|logit调整]]一样是为了处理类别不平衡问题而产生的，也就是我们所说的长尾学习。

## 有效样本数 (Effective Number of Samples)

有效样本数是一个衡量样本实际贡献程度的指标。传统的样本数假设每个样本对模型训练的贡献是相等的，但在类别不平衡的情况下，这一假设可能会导致模型过度关注多数类样本。

有效样本数的核心思想是考虑数据之间的重叠问题。在现实数据集中，特别是视觉数据中，样本之间通常会有一定程度的相似性或重叠。于是我们通过将每个样本视为一个小的邻域而不仅仅是一个点，计算有效样本数时可以更好地反映样本的真实信息量。

而随着样本数量的增加，每个新增样本对模型性能的边际贡献会逐渐减少。这是因为在相似的数据集中，新增加的样本很可能与已有样本重叠。有效样本数通过引入重叠概率，量化了这种边际效益递减的现象。

因此我们使用随机覆盖问题（random covering problem）的理论框架来定义和计算有效样本数。定义一个样本覆盖区域，并通过考虑样本之间的重叠概率，推导出有效样本数的计算公式。

有效样本数的定义为
$$N_{\text{eff}} = \frac{1 - \beta^n}{1 - \beta}$$
其中，$n$ 是某个类别的样本数量，$\beta$ 是一个小于1的超参数，用于控制对样本数的缩放。在极端情况下，当 $n$ 趋于无穷大时，有效样本数趋近于 $\frac{1}{1 - \beta}$。这一公式表明，有效样本数对大类别进行缩放，从而减少其对损失的影响。

## 类别平衡损失 (Class Balanced Loss)

类别平衡损失是一种通过考虑类别有效样本数来重新加权损失函数的方法。其目的是减轻类别不平衡带来的影响，使得模型在训练过程中能够更加平等地对待各个类别。他通过引入一个权重因子，该因子与有效样本数成反比。常用损失函数的类别平衡版本包括Softmax、Sigmoid和Focal Loss

类别平衡损失的公式为：

$$\text{CB\_Loss} = \frac{1 - \beta}{1 - \beta^{n_y}} \cdot \text{Loss}(y, \hat{y})$$
其中，$n_y$ 是真实类别 $y$ 的样本数，$\text{Loss}(y, \hat{y})$ 是传统的损失函数（如交叉熵损失）。 这个公式通过乘以一个权重项 $\frac{1 - \beta}{1 - \beta^{n_y}}$ 来调整每个类别的损失，从而使得稀有类别的样本在损失函数中占有更大的权重，减少类别不平衡对模型训练的不利影响。

## 代码实现

```python
beta = 0.99

# 计算有效样本数
effective_num = (1.0 - torch.pow(beta, sample_counts)) / (1.0 - beta)

# 计算类别平衡权重
cb_weights = 1.0 / effective_num
cb_weights = cb_weights / torch.sum(cb_weights) * len(sample_counts)

# 创建交叉熵损失函数并应用类别平衡权重
criterion = nn.CrossEntropyLoss(weight=cb_weights)
```
