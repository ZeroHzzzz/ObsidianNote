正则化（Regularization）是在机器学习和深度学习中用来防止模型**过拟合**的一种技术，指的是通过引入噪声或限制模型的复杂度，降低模型对输入或者参数的敏感性，避免过拟合，提高模型的泛化能力。常用的正则化方法包括约束目标函数(等价于约束模型参数)、约束网络结构、约束优化过程。

-   约束目标函数：在目标函数中增加模型参数的正则化项，包括L2正则化, L1正则化, 弹性网络正则化, L0正则化, 谱正则化, 自正交性正则化, WEISSI正则化, 梯度惩罚
-   约束网络结构：在网络结构中添加噪声，包括随机深度, Dropout及其系列方法,
-   约束优化过程：在优化过程中施加额外步骤，包括数据增强, 梯度裁剪, Early Stop, 标签平滑, 变分信息瓶颈, 虚拟对抗训练, Flooding

对于线性回归模型，**使用L1正则化的模型叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）**

## 约束目标函数

### L2正则化



L2正则化通过向损失函数中添加参数权重平方和的惩罚项，使得模型的参数尽可能小。这种惩罚会使得模型的权重更加平滑，从而减少模型对训练数据的过拟合倾向。

L2正则化的强度由超参数 $\lambda$ 控制。选择合适的 $\lambda$ 值非常重要，通常通过交叉验证来确定最佳的 $\lambda$ 值。如果 $\lambda$ 过大，正则化效果过强，模型可能会欠拟合；如果 $\lambda$ 过小，正则化效果不足，模型可能会过拟合

## L1正则化

随着海量数据处理的兴起，工程上对于模型稀疏化的要求也随之出现了。这时候，L2正则化已经不能满足需求，因为它只是使得模型的参数值趋近于0，而不是等于0，这样就无法丢掉模型里的任何一个特征，因此无法做到稀疏化。这时，L1的作用随之显现。L1正则化的作用是使得大部分模型参数的值等于0，这样一来，当模型训练好后，这些权值等于0的特征可以省去，从而达到稀疏化的目的，也节省了存储的空间，因为在计算时，值为0的特征都可以不用存储了。
