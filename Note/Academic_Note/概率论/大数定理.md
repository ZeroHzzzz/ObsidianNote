大数定理简单来说，指得是某个随机事件在单次试验中可能发生也可能不发生，但在大量重复实验中往往呈现出明显的规律性，即该随机事件发生的频率会向某个常数值收敛，该常数值即为该事件发生的概率。

另一种表达方式为当样本数据无限大时，样本均值趋于总体均值。

大数定理严格的数学定义分为两种，一种是弱大数定理，另一种是强大数定理

# 依概率收敛

**依概率收敛**是一种描述随机变量序列收敛行为的概念，表示随着样本量的增加，随机变量以概率的方式接近某个常数或随机变量。

$$ X*n \xrightarrow{P} X \quad \text{当且仅当} \quad \forall \epsilon > 0, \ \lim*{n \to \infty} \mathbb{P}(|X_n - X| \geq \epsilon) = 0 $$
这意味着：随着 $n$ 的增大，随机变量 $X_n$ 和 $X$ 之间的偏差 $|X_n - X|$ 的概率趋近于 0。

# 弱大数定理

设 $X_1, X_2, \dots, X_n$ 是一组独立同分布（i.i.d.）的随机变量，且它们的期望为 $\mu$，即 $\mathbb{E}[X_i] = \mu$，方差有限。定义样本均值为： $$ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i $$ 那么，样本均值 $\bar{X}_n$ 依概率收敛于总体期望 $\mu$，即： $$ \bar{X}\_n \xrightarrow{P} \mu $$
弱大数定理的核心思想是，**随着样本数的增加，样本均值越来越接近总体期望值**，而这个过程是以概率的形式发生的。也就是说，样本均值和期望值之间的偏差变得越来越小，且这种偏差变得不太可能发生。

# 强大数定理

强大数定理与弱大数定理的主要区别在于收敛的类型：弱大数定理给出的结论是“**以概率收敛**”于期望值 $\mu$，而强大数定理则表明样本均值**几乎必然收敛**于 $\mu$。

强大数定理主要有三个版本

## 伯努利大数定理

从定义概率的角度，**揭示了概率与频率的关系**，当N很大的时候，事件A发生的概率等于A发生的频率。

设fn为n重伯努利实验中事件A发生的次数，p为A在每次实验中发生的概率，则对任意给定的实数ε>0，有

$$\lim_{n\to\infty}P\left\{\left|\frac{f_{A}}{n}-p\right|<\epsilon\right\}=1$$

## 辛钦大数定理

辛钦大数定律从理论上指出：用算术平均值来近似实际真值是合理的。

设X1,X2,⋯是独立同分布（iid）的随机变量序列，且它们的期望值存在，记为E(Xi)=μ（同分布隐含条件即为期望相同），则对于任意的ɛ>0，有

$$\operatorname*{lim}_{n\to\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\frac{1}{n}\sum_{i=1}^{n}E\left(X_{i}\right)\right|<\varepsilon\right\}=1$$
当Xi为服从0-1分布的随机变量时，**辛钦大数定律就是伯努利大数定律**，故伯努利大数定律是辛钦伯努利大数定律的一个特例。

## 切比雪夫大数定律

设X1,X2,⋯是相互独立的随机变量序列，且它们的期望值存在，记为E(Xi)=μ(i=1,2,⋯)，有方差存在且有共同有限上界

，则对 任意的ɛ>0，有
$$D(X_{i})=\sigma_{i}^{2}<M$$
$$\lim_{n \to \infty} P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - \frac{1}{n} \sum_{i=1}^{n} u_i \right| < \varepsilon \right\} = 1$$
特别的，若Xi有相同的期望 μ ，则有
$$\lim_{n \to \infty} P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - \mu \right| < \varepsilon \right\} = 1$$

## 总结

| 大数定理 | 分布       | 期望 | 方差       | 用途     |
| -------- | ---------- | ---- | ---------- | -------- |
| 伯努利   | 二项分布   | 相同 | 相同       | 估算概率 |
| 辛钦     | 独立同分布 | 相同 | 相同       | 估算期望 |
| 切比雪夫 | 独立       | 存在 | 存在、有限 | 估算期望 |
